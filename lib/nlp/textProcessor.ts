/**
 * í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ
 * ë¬¸ì¥ ë¶„ë¦¬, í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ, ì¤‘ë³µ ì œê±° ë“±
 */

import { tokenize, detectLanguage } from './koreanTokenizer';
import { calculateAllSentenceScores } from './tfidf';
import { logger } from '@/lib/utils/logger';

// =====================================================
// Types
// =====================================================

export interface ScoredSentence {
  text: string;
  score: number;
  keywords: string[];
  position: number; // 0-1 ì‚¬ì´ ê°’ (ì›ë¬¸ ë‚´ ìœ„ì¹˜)
}

export interface ProcessedText {
  originalLength: number;
  sentences: ScoredSentence[];
  topSentences: string[];
  language: 'ko' | 'en' | 'mixed';
  extractionRatio: number;
}

// =====================================================
// ë¬¸ì¥ ë¶„ë¦¬
// =====================================================

/**
 * í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
 * í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì§€ì›
 */
export function splitSentences(text: string): string[] {
  // ë¬¸ì¥ ì¢…ê²° íŒ¨í„´: ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ + ê³µë°± ë˜ëŠ” ì¤„ë°”ê¿ˆ
  const sentencePattern = /[.!?ã€‚]+[\s\n]+|[.!?ã€‚]+$/g;

  // ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¨¼ì € ë¶„ë¦¬ (ë¬¸ë‹¨ êµ¬ë¶„)
  const paragraphs = text.split(/\n+/);

  const sentences: string[] = [];

  for (const paragraph of paragraphs) {
    const trimmed = paragraph.trim();
    if (!trimmed) continue;

    // ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ë¡œ ë¶„ë¦¬
    const parts = trimmed.split(sentencePattern);

    for (const part of parts) {
      const sentence = part.trim();
      // ìµœì†Œ 10ì ì´ìƒì¸ ë¬¸ì¥ë§Œ í¬í•¨
      if (sentence.length >= 10) {
        sentences.push(sentence);
      }
    }
  }

  return sentences;
}

// =====================================================
// ìœ ì‚¬ë„ ê³„ì‚° ë° ì¤‘ë³µ ì œê±°
// =====================================================

/**
 * ë‘ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard similarity)
 */
function calculateSimilarity(tokens1: string[], tokens2: string[]): number {
  if (tokens1.length === 0 || tokens2.length === 0) return 0;

  const set1 = new Set(tokens1);
  const set2 = new Set(tokens2);

  const intersection = new Set([...set1].filter((x) => set2.has(x)));
  const union = new Set([...set1, ...set2]);

  return intersection.size / union.size;
}

/**
 * ìœ ì‚¬í•œ ë¬¸ì¥ ì¤‘ë³µ ì œê±° (80% ì´ìƒ ìœ ì‚¬ë„)
 */
export function removeDuplicateSentences(
  sentences: ScoredSentence[],
  threshold: number = 0.8
): ScoredSentence[] {
  if (sentences.length === 0) return [];

  const result: ScoredSentence[] = [];
  const tokenCache = new Map<string, string[]>();

  // í† í° ìºì‹±
  for (const sentence of sentences) {
    tokenCache.set(sentence.text, tokenize(sentence.text));
  }

  for (const sentence of sentences) {
    const tokens = tokenCache.get(sentence.text)!;
    let isDuplicate = false;

    for (const existing of result) {
      const existingTokens = tokenCache.get(existing.text)!;
      const similarity = calculateSimilarity(tokens, existingTokens);

      if (similarity >= threshold) {
        isDuplicate = true;
        break;
      }
    }

    if (!isDuplicate) {
      result.push(sentence);
    }
  }

  return result;
}

// =====================================================
// í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
// =====================================================

/**
 * ë™ì ìœ¼ë¡œ ì¶”ì¶œí•  ë¬¸ì¥ ê°œìˆ˜ ê³„ì‚°
 * - í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜: min(ë¬¸ì¥ìˆ˜, max(10, ë¬¸ì¥ìˆ˜ * 0.4))
 */
function calculateExtractCount(totalSentences: number): number {
  if (totalSentences <= 5) return totalSentences;
  return Math.min(totalSentences, Math.max(10, Math.floor(totalSentences * 0.4)));
}

/**
 * ìƒìœ„ Nê°œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
 */
export function extractTopSentences(
  sentences: ScoredSentence[],
  count?: number
): ScoredSentence[] {
  if (sentences.length === 0) return [];

  const extractCount = count ?? calculateExtractCount(sentences.length);

  // ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
  const sorted = [...sentences].sort((a, b) => b.score - a.score);

  // ìƒìœ„ Nê°œ ì„ íƒ
  const top = sorted.slice(0, extractCount);

  // ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬ (position ê¸°ì¤€)
  return top.sort((a, b) => a.position - b.position);
}

// =====================================================
// ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
// =====================================================

/**
 * í…ìŠ¤íŠ¸ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
 * 1. ë¬¸ì¥ ë¶„ë¦¬
 * 2. í† í°í™” ë° TF-IDF ì ìˆ˜ ê³„ì‚°
 * 3. ì¤‘ë³µ ì œê±°
 * 4. í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
 */
export function processText(text: string): ProcessedText {
  const startTime = Date.now();
  const originalLength = text.length;
  const language = detectLanguage(text);

  logger.debug('NLP', `ì–¸ì–´ ê°ì§€: ${language}`);

  // 1. ë¬¸ì¥ ë¶„ë¦¬
  const splitStart = Date.now();
  const rawSentences = splitSentences(text);
  logger.debug('NLP', `ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ (${Date.now() - splitStart}ms)`, {
    'ë¬¸ì¥ ìˆ˜': rawSentences.length,
  });

  if (rawSentences.length === 0) {
    logger.warn('NLP', 'ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ');
    return {
      originalLength,
      sentences: [],
      topSentences: [],
      language,
      extractionRatio: 0,
    };
  }

  // 2. í† í°í™”
  const tokenStart = Date.now();
  const tokenizedSentences = rawSentences.map((s) => tokenize(s));
  const totalTokens = tokenizedSentences.reduce((sum, t) => sum + t.length, 0);
  logger.debug('NLP', `í† í°í™” ì™„ë£Œ (${Date.now() - tokenStart}ms)`, {
    'ì´ í† í° ìˆ˜': totalTokens,
    'í‰ê·  í† í°/ë¬¸ì¥': Math.round(totalTokens / rawSentences.length),
  });

  // 3. TF-IDF ì ìˆ˜ ê³„ì‚°
  const tfidfStart = Date.now();
  const scores = calculateAllSentenceScores(tokenizedSentences);
  logger.debug('NLP', `TF-IDF ê³„ì‚° ì™„ë£Œ (${Date.now() - tfidfStart}ms)`);

  // 4. ScoredSentence ë°°ì—´ ìƒì„±
  const scoredSentences: ScoredSentence[] = rawSentences.map((text, index) => ({
    text,
    score: scores[index] || 0,
    keywords: tokenizedSentences[index].slice(0, 5), // ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
    position: index / rawSentences.length,
  }));

  // 5. ì¤‘ë³µ ì œê±°
  const dedupeStart = Date.now();
  const uniqueSentences = removeDuplicateSentences(scoredSentences);
  const removedCount = scoredSentences.length - uniqueSentences.length;
  logger.debug('NLP', `ì¤‘ë³µ ì œê±° ì™„ë£Œ (${Date.now() - dedupeStart}ms)`, {
    'ì œê±°ëœ ë¬¸ì¥': removedCount,
    'ë‚¨ì€ ë¬¸ì¥': uniqueSentences.length,
  });

  // 6. í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
  const extractStart = Date.now();
  const topSentences = extractTopSentences(uniqueSentences);
  logger.debug('NLP', `í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì™„ë£Œ (${Date.now() - extractStart}ms)`, {
    'ì¶”ì¶œ ë¬¸ì¥ ìˆ˜': topSentences.length,
  });

  // 7. ì¶”ì¶œ ë¹„ìœ¨ ê³„ì‚°
  const extractedLength = topSentences.reduce((sum, s) => sum + s.text.length, 0);
  const extractionRatio = extractedLength / originalLength;

  const totalDuration = Date.now() - startTime;
  logger.info('NLP', `ğŸ“Š í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ (${totalDuration}ms)`, {
    'ì›ë³¸': `${originalLength}ì`,
    'ì²˜ë¦¬ í›„': `${extractedLength}ì`,
    'ì••ì¶•ë¥ ': `${Math.round((1 - extractionRatio) * 100)}%`,
    'ë¬¸ì¥': `${rawSentences.length} â†’ ${topSentences.length}`,
  });

  return {
    originalLength,
    sentences: scoredSentences,
    topSentences: topSentences.map((s) => s.text),
    language,
    extractionRatio,
  };
}

/**
 * í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„íˆ ê¸´ì§€ í™•ì¸ (ì „ì²˜ë¦¬ í•„ìš” ì—¬ë¶€)
 * 500ì ë¯¸ë§Œì´ë©´ ì „ì²˜ë¦¬ ë¶ˆí•„ìš”
 */
export function shouldPreprocess(text: string): boolean {
  return text.length >= 500;
}
